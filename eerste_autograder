import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import SGDRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import cross_val_score

import category_encoders as ce


# These are your training samples along with their labels
data = pd.read_csv('/kaggle/input/health-insurance/health_insurance_train.csv')

X = data.drop('whrswk', axis=1)
y = data['whrswk']

print(data.head())

# Kijken voor afwijkende data
cols = ['whrswk', 'hhi', 'whi', 'hhi2', 'education', 'race','hispanic',
        'experience', 'kidslt6', 'kids618', 'husby', 'region']

for col in cols:
    zeros = (data[col] == 0).sum()
    others = (data[col] == 'other').sum()
    nans = data[col].isna().sum()
    print(f"{col}: {zeros} nullen, {nans} NaN's, {others} other")



Pipeline 1

Num = ['experience', 'kidslt6', 'kids618', 'husby']
Bin_cat = ['hhi', 'whi', 'hhi2', 'hispanic']
Multi_cat = ['race', 'region', 'education']


num_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])


cat_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),
    ('scaler', StandardScaler())
])


prep_1 = ColumnTransformer(
    transformers=[
        ('num_scaled', num_pipe, Num),
        ('bin_cat_ohe', cat_pipe, Bin_cat), 
        ('multi_cat_ohe', cat_pipe, Multi_cat) 
    ],
    remainder='drop'
)
X_transformed = prep_1.fit_transform(X)

print(f"1. Initieel aantal rijen en kolommen (X): {X.shape}")
print(f"   Aantal rijen en kolommen na P1:        {X_transformed.shape}")

# Gebruik .get_feature_names_out() om de kolomnamen te zien.
try:
    feature_names = prep_1.get_feature_names_out()
    print(f"\n2. Totaal aantal features na P1: {len(feature_names)}")
    print("   Alle feature namen:")
    print(feature_names)
except Exception as e:
    print(f"\nKon featurenamen niet ophalen: {e}")

X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)
#print("\n3. Eerste 5 rijen van de getransformeerde data (Controle Scaling/OHE):")
#print(X_transformed_df.head())



Pipeline 2

def feature_engineer_p2(X_df):
    """Voegt gewogen kinderlast, kwadratisch inkomen, en onverzekerdheid toe."""
    X_fe = X_df.copy()
    
    # 1. Gewogen Kinderlast (Child Burden)
    X_fe['child_burden'] = 2 * X_fe['kidslt6'] + X_fe['kids618']
    
    # 2. Kwadratische Term voor Husby (om non-lineariteit vast te leggen)
    # Husby is in duizenden dollars, dus de kwadratische term is zinvol.
    X_fe['husby_sq'] = X_fe['husby'] ** 2
    
    # 3. Onverzekerdheid (Sterke motivator om te werken)
    # 1 als hhi='no' EN whi='no', anders 0.
    X_fe['uninsured'] = ((X_fe['hhi'] == 'no') & (X_fe['whi'] == 'no')).astype(int)
    
    return X_fe

X_fe = feature_engineer_p2(X) # Voer de FE uit op de volledige dataset X

# De oorspronkelijke kidslt6/kids618 vallen af omdat ze al in child_burden zitten.
NUM_P2_FE = ['experience', 'husby', 'husby_sq', 'child_burden'] # De 4 features die bewerkt zijn
NUM_P2_PASSTHROUGH = ['education'] # Niet-gefeateerde numerieke/ordinale features
PFE_TARGETS = ['husby', 'experience'] # De features die polynomiale interactie krijgen
num_pipe2 = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median'))
])

cat_pipe2 = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))
])


# 1. Pipeline voor Polynomial Feature Extraction (alleen interacties)
pfe_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('scaler', StandardScaler())
])

# 2. Pipeline voor Numeriek Passthrough (voor de rest van de numerieke features)
num_pipe_passthrough = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

prep_2 = ColumnTransformer(
    transformers=[
        # PFE/Interactie op de kritische features
        ('pfe_feats', pfe_pipe, PFE_TARGETS),
        
        # Numeriek Passthrough (incl. child_burden, husby_sq, uninsured)
        ('num_pass', num_pipe_passthrough, NUM_P2_FE),
        
        # Categorische (Gebruikt cat_pipe2 uit uw code)
        ('bin_cat_ohe', cat_pipe2, Bin_cat),
        ('multi_cat_ohe', cat_pipe2, Multi_cat)
    ],
    remainder='drop'
)


Part B - Regression with Default Hyperparameters


models_p1 = {
    'KNN Regression': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', KNeighborsRegressor(n_neighbors=5))
    ]),
    'SGD Linear Regression': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', SGDRegressor(random_state=42))
    ]),   
    'Random Forest': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)) # n_jobs=-1 voor snellere training
    ]),    
    'Regression Tree': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', DecisionTreeRegressor(random_state=42))
    ])
}


models_p2 = {
    'KNN Regression (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', KNeighborsRegressor(n_neighbors=5))
    ]),
    'SGD Linear Regression (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', SGDRegressor(random_state=42))
    ]),
    'Random Forest (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))
    ]),
    'Regression Tree (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', DecisionTreeRegressor(random_state=42))
    ])
}

baseline_guess = y.median()

y_baseline_pred = np.full_like(y, baseline_guess)
mae_baseline = mean_absolute_error(y, y_baseline_pred)

print("--- Baseline Model MAE ---")
print(f"De mediane uren gewerkt (Baseline Gok): {baseline_guess:.2f} uur")
print(f"De MAE van deze Baseline Gok: {mae_baseline:.2f} uur")

all_models = {**models_p1, **models_p2}
results_mae = {}
K_FOLDS = 5

for name, model in all_models.items():
    print(f"Start CV voor: {name}...")
    
    # BELANGRIJK: Gebruik X_fe (met de nieuwe features) voor P2-modellen, X voor P1-modellen.
    # Dit is essentieel voor correcte Feature Engineering.
    data_input = X_fe if 'P2' in name else X
    
    scores = cross_val_score(
        model, 
        data_input, y, 
        scoring='neg_mean_absolute_error', 
        cv=K_FOLDS, 
        n_jobs=-1
    )
    
    mean_mae = -scores.mean()
    std_mae = scores.std()
    results_mae[name] = mean_mae
    
    print(f"âœ… {name:<30} - Gem. MAE: {mean_mae:.2f} uur (+/- {std_mae:.2f} uur)")

print("\n--- Samenvatting van de Generalisatieprestaties ---")
results_df = pd.Series(results_mae).sort_values()
print(results_df)





data_autograder = pd.read_csv('/kaggle/input/autograder/health_insurance_autograde.csv')
data_autograder.head()

# TODO Replace this with your own estimate of the MAE of your best model
estimate_MAE_on_new_data = np.array([1.0])

# TODO Replace this with the predictions of your best model
# via e.g. prediction = model.predict(data_autograder)
predictions_autograder_data = np.array([-1] * 17272)

# Upload this file to the Vocareum autograder:
result = np.append(estimate_MAE_on_new_data, predictions_autograder_data)
pd.DataFrame(result).to_csv("autograder_submission.txt", index=False, header=False)
