import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import SGDRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, make_scorer
from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split
from sklearn.base import clone

# These are your training samples along with their labels
data = pd.read_csv('/kaggle/input/health-insurance/health_insurance_train.csv')

X = data.drop('whrswk', axis=1)
y = data['whrswk']

print(data.head())
# You need to extract the features and the regression target. The regression target is 'whrswk'. 

cols = ['whrswk', 'hhi', 'whi', 'hhi2', 'education', 'race','hispanic',
        'experience', 'kidslt6', 'kids618', 'husby', 'region']

for col in cols:
    zeros = (data[col] == 0).sum()
    others = (data[col] == 'other').sum()
    nans = data[col].isna().sum()
    print(f"{col}: {zeros} zeroes, {nans} NaN's, {others} other")

Num = ['experience', 'kidslt6', 'kids618', 'husby']
Bin_cat = ['hhi', 'whi', 'hhi2', 'hispanic']
Multi_cat = ['race', 'region', 'education']


num_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy = 'median')),
    ('scaler', StandardScaler())
])

cat_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy = 'most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse_output = False)),
    ('scaler', StandardScaler())
])

prep_1 = ColumnTransformer(
    transformers=[
        ('num_scaled', num_pipe, Num),
        ('bin_cat_ohe', cat_pipe, Bin_cat), 
        ('multi_cat_ohe', cat_pipe, Multi_cat) 
    ],
    remainder = 'drop'
)

X_transformed = prep_1.fit_transform(X)

print(f"1. Initieel aantal rijen en kolommen (X): {X.shape}")
print(f"   Aantal rijen en kolommen na P1:        {X_transformed.shape}")

# .get_feature_names_out() om de getransformeerde kolomnamen te zien
try:
    feature_names = prep_1.get_feature_names_out()
    print(f"\n2. Totaal aantal features na P1: {len(feature_names)}")
    print("   Alle feature namen:")
    print(feature_names)
except Exception as e:
    print(f"\nKon featurenamen niet ophalen: {e}")

X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)
#print("\n3. Eerste 5 rijen van de getransformeerde data (Controle Scaling/OHE):")
#print(X_transformed_df.head())

def feature_engineer_p2(X_df):
    X_f = X_df.copy()
    
    # Jongere kinderen hebben meer aandacht nodig dan oudere kinderen 
    X_f['child_burden'] = 2 * X_f['kidslt6'] + X_f['kids618']
    
    # Kwadratische Term voor Husby om non-lineariteit vast te leggen
    X_f['husby_sq'] = X_f['husby'] ** 2
    
    # Man en vrouw hebben beide verzekering door werk vrouw, indicatie vam goede baan vrouw
    X_f['dual_job_insurance'] = ((X_f['whi'] == 'yes') & (X_f['hhi2'] == 'yes')).astype(int)
    
    return X_f

X_f = feature_engineer_p2(X)

# Werkervaring vrouw correlatie met inkomen van man
FE_targets = ['husby', 'experience'] 

# Dit zijn de NIEUWE, numerieke features die geschaald moeten worden
# kidslt6/kids618 en whi/hhi2 vallen af omdat ze nu in resp child_burden en dual_job_insurance zitten
Num_f = ['husby_sq', 'child_burden', 'dual_job_insurance']


num_pipe2 = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy = 'median'))
])

cat_pipe2 = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy = 'most_frequent')),
    ('onehot', OneHotEncoder(drop = 'first', handle_unknown = 'ignore', sparse_output = False))
])


# Polynomial Feature Extraction (alleen interacties)
pfe_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy = 'median')),
    ('poly', PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)),
    ('scaler', StandardScaler())
])

# Pipeline voor Numeriek Passthrough (voor de rest van de numerieke features)
num_pipe_passthrough = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy = 'median')),
    ('scaler', StandardScaler())
])

prep_2 = ColumnTransformer(
    transformers=[
        # PFE op de kritische features (husby, experience)
        ('pfe_feats', pfe_pipe, FE_targets),
        
        # Numeriek Passthrough op de nieuwe FE features
        ('num_pass', num_pipe_passthrough, Num_f), 
        
        ('bin_cat_ohe', cat_pipe2, Bin_cat),
        ('multi_cat_ohe', cat_pipe2, Multi_cat)
    ],
    remainder = 'drop'
)

models_p1 = {
    'KNN Regression': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', KNeighborsRegressor(n_neighbors = 5))
    ]),
    'SGD Linear Regression': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', SGDRegressor(random_state = 42))
    ]),   
    'Random Forest': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', RandomForestRegressor(n_estimators = 100, random_state = 42, n_jobs = -1)) # n_jobs=-1 voor snellere training
    ]),    
    'Regression Tree': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', DecisionTreeRegressor(random_state = 42))
    ])
}


models_p2 = {
    'KNN Regression (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', KNeighborsRegressor(n_neighbors = 5))
    ]),
    'SGD Linear Regression (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', SGDRegressor(random_state = 42))
    ]),
    'Random Forest (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', RandomForestRegressor(n_estimators = 100, random_state = 42, n_jobs = -1))
    ]),
    'Regression Tree (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', DecisionTreeRegressor(random_state = 42))
    ])
}


baseline_guess = y.median()
y_baseline_pred = np.full_like(y, baseline_guess)
mae_baseline = mean_absolute_error(y, y_baseline_pred)

print("--- Baseline Model MAE ---")
print(f"Gok: de mediane uren gewerkt: {baseline_guess:.2f} uur")
print(f"De MAE van deze Baseline Gok: {mae_baseline:.2f} uur")

all_models = {**models_p1, **models_p2}
results_mae = {}
K_FOLDS = 5

default_results_p2 = {}

for name, model in all_models.items():
    print(f"Start CV voor: {name}...")
    
    # Gebruik X_f voor P2-modellen, X voor P1-modellen.
    data_input = X_f if 'P2' in name else X
    
    scores = cross_val_score(
        model, 
        data_input, y, 
        scoring = 'neg_mean_absolute_error', 
        cv = K_FOLDS, 
        n_jobs = -1
    )
    
    mean_mae = -scores.mean()
    
    # Sla alle resultaten op (voor de samenvatting)
    results_mae[name] = mean_mae 
    
    # Sla ALLEEN de Pipeline 2 resultaten op (voor vergelijking in Stap 11)
    if 'P2' in name:
        default_results_p2[name] = mean_mae 
        
    print(f"{name:<30} - Gem. MAE: {mean_mae:.2f} uur)\n")

print("\n--- MAE prestaties gesorteerd ---")
results_df = pd.Series(results_mae).sort_values()
print(results_df)

print("\n--- Opgeslagen Default P2 MAE's ---")
print(pd.Series(default_results_p2).to_markdown())

prep = prep_2 
tuned_models = {}

par_grids = {
    'KNN Regression (P2)': {
        'regressor__n_neighbors': [13],
        'regressor__weights': ['uniform']
    },
    'SGD Linear Regression (P2)': {
        'regressor__alpha': [0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000001],
        'regressor__penalty': ['l2']
    },
    'Random Forest (P2)': {
        'regressor__n_estimators': [147],
        'regressor__max_depth': [7]
    },
    'Regression Tree (P2)': {
        'regressor__max_depth': [10],
        'regressor__min_samples_leaf': [84]
    }
}

for name, par in par_grids.items():
    
    orig_pipeline = models_p2[name]
    base_pipeline = clone(orig_pipeline)

    grid_search = GridSearchCV(
        estimator = base_pipeline,
        param_grid = par,
        scoring = 'neg_mean_absolute_error',
        cv = 5, n_jobs = -1, verbose = 1
    )
    
    print(f"\nStart GridSearch voor: {name}")
    grid_search.fit(X_f, y)
    
    tuned_models[name] = grid_search
    
    print(f"Beste parameters voor {name}: {grid_search.best_params_}")
    print(f"Beste CV MAE: {-grid_search.best_score_:.4f} uur")

results_df = pd.DataFrame(grid_search.cv_results_)


analysis_cols = [
    'params', 
    'mean_test_score', 
    'std_test_score', 
    'rank_test_score'
]


analysis_df = results_df[analysis_cols].sort_values(by = 'rank_test_score')
analysis_df['mean_MAE'] = -analysis_df['mean_test_score']

X_train_curve, X_test_curve, y_train_curve, y_test_curve = train_test_split(
    X_f, y, test_size = 0.1, random_state = 42
)

sgd_grid_result = tuned_models['SGD Linear Regression (P2)']
best_sgd_pipeline = sgd_grid_result.best_estimator_

sgd_regressor_params = best_sgd_pipeline.named_steps['regressor'].get_params()

new_sgd_regressor = SGDRegressor(
    **sgd_regressor_params
)

prep_transform = best_sgd_pipeline.named_steps['preprocessor']
X_train_transformed = prep_transform.fit_transform(X_train_curve)
X_test_transformed = prep_transform.transform(X_test_curve)

epochs = 100
mae_history = []

for epoch in range(epochs):
    new_sgd_regressor.partial_fit(X_train_transformed, y_train_curve)
    
    y_pred = new_sgd_regressor.predict(X_test_transformed)
    mae = mean_absolute_error(y_test_curve, y_pred)
    mae_history.append(mae)


print(f"Start MAE: {mae_history[0]:.4f} uur")
print(f"Eind MAE na {epochs} epochs: {mae_history[-1]:.4f} uur")

plt.figure(figsize=(8, 4))
plt.plot(range(len(mae_history)), mae_history, marker='o', markersize=3, linestyle='-', color='blue')
plt.title('SGD Regressor: MAE per Epoch')
plt.xlabel('Epochs')
plt.ylabel('Mean Absolute Error (MAE)')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

comparison_data = []
best_overall_mae = float('inf')
best_overall_model = ""

for name, mae_default in default_results_p2.items():
    mae_tuned = tuned_results.get(name, float('nan'))
    
    if not np.isnan(mae_tuned): 
        if mae_tuned < best_overall_mae:
            best_overall_mae = mae_tuned
            best_overall_model = name
            
        improvement = mae_default - mae_tuned
        
        comparison_data.append({
            'Model': name,
            'Default MAE': f"{mae_default:.4f}",
            'Tuned MAE': f"{mae_tuned:.4f}",
            'Improvement': f"{improvement:.4f}"
        })
    else:
        comparison_data.append({
            'Model': name,
            'Default MAE': f"{mae_default:.4f}",
            'Tuned MAE': 'N/A (Tuning Mislukt)',
            'Improvement': 'N/A'
        })

comparison_df = pd.DataFrame(comparison_data)
print(comparison_df.to_markdown(index = False))

print(f"\nBest model after hyperparameter tuning is: {best_overall_model} with a MAE of {best_overall_mae:.4f}.")
