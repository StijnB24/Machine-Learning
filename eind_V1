import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, PolynomialFeatures
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import SGDRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error, make_scorer
from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split
from sklearn.base import clone

import category_encoders as ce

import matplotlib.pyplot as plt

# These are your training samples along with their labels
data = pd.read_csv('/kaggle/input/health-insurance/health_insurance_train.csv')

X = data.drop('whrswk', axis=1)
y = data['whrswk']

print(data.head())
# You need to extract the features and the regression target. The regression target is 'whrswk'. 

cols = ['whrswk', 'hhi', 'whi', 'hhi2', 'education', 'race','hispanic',
        'experience', 'kidslt6', 'kids618', 'husby', 'region']

for col in cols:
    zeros = (data[col] == 0).sum()
    others = (data[col] == 'other').sum()
    nans = data[col].isna().sum()
    print(f"{col}: {zeros} nullen, {nans} NaN's, {others} other")


Num = ['experience', 'kidslt6', 'kids618', 'husby']
Bin_cat = ['hhi', 'whi', 'hhi2', 'hispanic']
Multi_cat = ['race', 'region', 'education']


num_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])


cat_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),
    ('scaler', StandardScaler())
])


prep_1 = ColumnTransformer(
    transformers=[
        ('num_scaled', num_pipe, Num),
        ('bin_cat_ohe', cat_pipe, Bin_cat), 
        ('multi_cat_ohe', cat_pipe, Multi_cat) 
    ],
    remainder='drop'
)

X_transformed = prep_1.fit_transform(X)

print(f"1. Initieel aantal rijen en kolommen (X): {X.shape}")
print(f"   Aantal rijen en kolommen na P1:        {X_transformed.shape}")

# Gebruik .get_feature_names_out() om de kolomnamen te zien.
try:
    feature_names = prep_1.get_feature_names_out()
    print(f"\n2. Totaal aantal features na P1: {len(feature_names)}")
    print("   Alle feature namen:")
    print(feature_names)
except Exception as e:
    print(f"\nKon featurenamen niet ophalen: {e}")

X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)
#print("\n3. Eerste 5 rijen van de getransformeerde data (Controle Scaling/OHE):")
#print(X_transformed_df.head())

def feature_engineer_p2(X_df):
    """Voegt gewogen kinderlast, kwadratisch inkomen, en onverzekerdheid toe."""
    X_fe = X_df.copy()
    
    # 1. Gewogen Kinderlast (Child Burden)
    X_fe['child_burden'] = 2 * X_fe['kidslt6'] + X_fe['kids618']
    
    # 2. Kwadratische Term voor Husby (om non-lineariteit vast te leggen)
    # Husby is in duizenden dollars, dus de kwadratische term is zinvol.
    X_fe['husby_sq'] = X_fe['husby'] ** 2
    
    # 3. Onverzekerdheid (Sterke motivator om te werken)
    # 1 als hhi='no' EN whi='no', anders 0.
    X_fe['uninsured'] = ((X_fe['hhi'] == 'no') & (X_fe['whi'] == 'no')).astype(int)
    
    return X_fe

X_fe = feature_engineer_p2(X) # Voer de FE uit op de volledige dataset X

# De oorspronkelijke kidslt6/kids618 vallen af omdat ze al in child_burden zitten.
NUM_P2_FE = ['experience', 'husby', 'husby_sq', 'child_burden'] # De 4 features die bewerkt zijn
NUM_P2_PASSTHROUGH = ['education'] # Niet-gefeateerde numerieke/ordinale features
PFE_TARGETS = ['husby', 'experience'] # De features die polynomiale interactie krijgen

num_pipe2 = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median'))
])

cat_pipe2 = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))
])


# 1. Pipeline voor Polynomial Feature Extraction (alleen interacties)
pfe_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('poly', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),
    ('scaler', StandardScaler())
])

# 2. Pipeline voor Numeriek Passthrough (voor de rest van de numerieke features)
num_pipe_passthrough = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

prep_2 = ColumnTransformer(
    transformers=[
        # PFE/Interactie op de kritische features
        ('pfe_feats', pfe_pipe, PFE_TARGETS),
        
        # Numeriek Passthrough (incl. child_burden, husby_sq, uninsured)
        ('num_pass', num_pipe_passthrough, NUM_P2_FE),
        
        # Categorische (Gebruikt cat_pipe2 uit uw code)
        ('bin_cat_ohe', cat_pipe2, Bin_cat),
        ('multi_cat_ohe', cat_pipe2, Multi_cat)
    ],
    remainder='drop'
)

models_p1 = {
    'KNN Regression': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', KNeighborsRegressor(n_neighbors=5))
    ]),
    'SGD Linear Regression': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', SGDRegressor(random_state=42))
    ]),   
    'Random Forest': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)) # n_jobs=-1 voor snellere training
    ]),    
    'Regression Tree': Pipeline(steps=[
        ('preprocessor', prep_1), 
        ('regressor', DecisionTreeRegressor(random_state=42))
    ])
}


models_p2 = {
    'KNN Regression (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', KNeighborsRegressor(n_neighbors=5))
    ]),
    'SGD Linear Regression (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', SGDRegressor(random_state=42))
    ]),
    'Random Forest (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))
    ]),
    'Regression Tree (P2)': Pipeline(steps=[
        ('preprocessor', prep_2), 
        ('regressor', DecisionTreeRegressor(random_state=42))
    ])
}

baseline_guess = y.median()

y_baseline_pred = np.full_like(y, baseline_guess)
mae_baseline = mean_absolute_error(y, y_baseline_pred)

print("--- Baseline Model MAE ---")
print(f"De mediane uren gewerkt (Baseline Gok): {baseline_guess:.2f} uur")
print(f"De MAE van deze Baseline Gok: {mae_baseline:.2f} uur")

all_models = {**models_p1, **models_p2}
results_mae = {}
K_FOLDS = 5

default_results_p2 = {}

for name, model in all_models.items():
    print(f"Start CV voor: {name}...")
    
    # BELANGRIJK: Gebruik X_fe (met de nieuwe features) voor P2-modellen, X voor P1-modellen.
    data_input = X_fe if 'P2' in name else X
    
    scores = cross_val_score(
        model, 
        data_input, y, 
        scoring='neg_mean_absolute_error', 
        cv=K_FOLDS, 
        n_jobs=-1
    )
    
    mean_mae = -scores.mean()
    std_mae = scores.std()
    
    # Sla alle resultaten op (voor de samenvatting)
    results_mae[name] = mean_mae 
    
    # Sla ALLEEN de Pipeline 2 resultaten op (voor Stap 11)
    if 'P2' in name:
        # We slaan ze op zonder de '(P2)' suffix voor de netheid, maar het is niet strikt nodig
        # Laten we ze opslaan met de volle naam omwille van de eenvoud.
        default_results_p2[name] = mean_mae 
        
    print(f"âœ… {name:<30} - Gem. MAE: {mean_mae:.2f} uur (+/- {std_mae:.2f} uur)\n")

print("\n--- Samenvatting van de Generalisatieprestaties ---")
results_df = pd.Series(results_mae).sort_values()
print(results_df)

print("\n--- Opgeslagen Default P2 MAE's ---")
print(pd.Series(default_results_p2).to_markdown())

prep = prep_2 
tuned_models = {}

par_grids = {
    'KNN Regression (P2)': {
        'regressor__n_neighbors': [3, 5, 7, 9, 11],
        'regressor__weights': ['uniform', 'distance']
    },
    'SGD Linear Regression (P2)': {
        'regressor__alpha': [0.00000000000000000000000000000000000000000000000000000000000000000001, 0.0000000000001, 0.00000001, 0.0000001],
        'regressor__penalty': ['l2', 'l1']
    },
    'Random Forest (P2)': {
        'regressor__n_estimators': [175,176,177],
        'regressor__max_depth': [7]
    },
    'Regression Tree (P2)': {
        'regressor__max_depth': [6],
        'regressor__min_samples_leaf': [24, 25, 26]
    }
}

print("\n" + "="*50)
print("--- 8. Tuning met GridSearch (Geoptimaliseerd) ---")
print("="*50)


for name, par in par_grids.items():
    
    orig_pipeline = models_p2[name]
    base_pipeline = clone(orig_pipeline)

    grid_search = GridSearchCV(
        estimator=base_pipeline,
        param_grid=par,
        scoring='neg_mean_absolute_error',
        cv = 5, n_jobs = -1, verbose = 1
    )
    
    print(f"\nStart GridSearch voor: {name}")
    grid_search.fit(X_fe, y)
    
    tuned_models[name] = grid_search
    
    print(f"Beste parameters voor {name}: {grid_search.best_params_}")
    print(f"Beste CV MAE: {-grid_search.best_score_:.4f} uur")

results_df = pd.DataFrame(grid_search.cv_results_)


analysis_cols = [
    'params', 
    'mean_test_score', 
    'std_test_score', 
    'rank_test_score'
]


analysis_df = results_df[analysis_cols].sort_values(by='rank_test_score')
analysis_df['mean_MAE'] = -analysis_df['mean_test_score']

print("\n" + "="*50)
print(f"Analyse van Parameters voor {name}")
print("="*50)

# Toon de top 5 parametercombinaties
print(analysis_df[['params', 'mean_MAE', 'std_test_score']].head().to_markdown(index=False))

tuned_results = {}

print("\n" + "="*50)
print("--- 9. Eerlijke Prestatieschatting (Getunede Modellen) ---")
print("Gebruikt de beste CV-score uit GridSearch.")
print("="*50)

for name, grid_search_object in tuned_models.items():
    # De 'best_score_' staat op het GridSearch object, NIET op de best_estimator_
    
    # 1. Haal de beste MAE op (wat de negatieve beste score is)
    mae = -grid_search_object.best_score_ 
    
    # 2. Optioneel: Haal de standaardafwijking op voor nauwkeurigheid
    # We zoeken de index van de beste score
    best_index = grid_search_object.best_index_
    std = grid_search_object.cv_results_['std_test_score'][best_index]
    
    tuned_results[name] = mae
    
    print(f"âœ… {name:<30} - Getunede CV MAE: {mae:.4f} uur (+/- {std:.4f} uur)")




X_train_curve, X_test_curve, y_train_curve, y_test_curve = train_test_split(
    X_fe, y, test_size=0.1, random_state=42
)

# 1. Haal de best_estimator_ (de gefitte pipeline) op uit het GridSearch object
sgd_grid_result = tuned_models['SGD Linear Regression (P2)']
best_sgd_pipeline = sgd_grid_result.best_estimator_

# 2. Isoleer en KLOON de SGD regressor parameters
sgd_regressor_params = best_sgd_pipeline.named_steps['regressor'].get_params()

# 3. Initialiseer de NIEUWE SGD Regressor
new_sgd_regressor = SGDRegressor(
    **sgd_regressor_params
)

# 4. Isoleer de preprocessor en transformeer de data
prep_transform = best_sgd_pipeline.named_steps['preprocessor']
X_train_transformed = prep_transform.fit_transform(X_train_curve)
X_test_transformed = prep_transform.transform(X_test_curve)

# 5. Voer de training in stappen uit (epochs)
epochs = 100
mae_history = []

for epoch in range(epochs):
    # partial_fit traint Ã©Ã©n keer over de data
    new_sgd_regressor.partial_fit(X_train_transformed, y_train_curve)
    
    # Meet prestatie (op de ongeziene test set)
    y_pred = new_sgd_regressor.predict(X_test_transformed)
    mae = mean_absolute_error(y_test_curve, y_pred)
    mae_history.append(mae)

print("\n" + "="*50)
print("--- 10. SGD Convergentie Check (MAE over Epochs) ---")
print("="*50)
print(f"Start MAE: {mae_history[0]:.4f} uur")
print(f"Eind MAE na {epochs} epochs: {mae_history[-1]:.4f} uur")

#Visualisatie voor uw rapport (uncomment om te plotten in uw notebook)
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))
plt.plot(range(len(mae_history)), mae_history, marker='o', markersize=3, linestyle='-', color='blue')
plt.title('SGD Regressor: MAE per Epoch (Convergentie)')
plt.xlabel('Epochs')
plt.ylabel('Mean Absolute Error (MAE)')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()
print("Plot de 'mae_history' om de SGD-convergentiecurve te visualiseren. ")

# --- 11. Vergelijking MAE: Default vs. Getuned (GEOPTIMALISEERD) ---

# Aangenomen dat 'tuned_results' is gevuld na Stap 9.
comparison_data = []
best_overall_mae = float('inf')
best_overall_model = ""

print("\n" + "="*50)
print("--- 11. Vergelijking MAE: Default vs. Getuned ---")
print("Gebruikt de opgeslagen Default P2 MAE's.")
print("="*50)

# We itereren over de default P2 resultaten
for name, mae_default in default_results_p2.items():
    
    # Haal de getunede score op
    mae_tuned = tuned_results.get(name, float('nan'))
    
    # Alleen vergelijken als de tuning succesvol was (niet nan)
    if not np.isnan(mae_tuned):
        
        if mae_tuned < best_overall_mae:
            best_overall_mae = mae_tuned
            best_overall_model = name
            
        improvement = mae_default - mae_tuned
        
        comparison_data.append({
            'Model': name,
            'Default MAE (P2)': f"{mae_default:.4f}",
            'Tuned MAE (P2)': f"{mae_tuned:.4f}",
            'Verbetering': f"{improvement:.4f}"
        })
    else:
        comparison_data.append({
            'Model': name,
            'Default MAE (P2)': f"{mae_default:.4f}",
            'Tuned MAE (P2)': 'N/A (Tuning Mislukt)',
            'Verbetering': 'N/A'
        })

comparison_df = pd.DataFrame(comparison_data)

# Print de vergelijkingstabel
print(comparison_df.to_markdown(index=False))

# Conclusie
print(f"\nHet beste model na hyperparameter tuning is: {best_overall_model} met een MAE van {best_overall_mae:.4f} uur.")
